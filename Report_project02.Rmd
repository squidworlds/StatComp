---
title: "Modelling Peak Electricity Demand in Great Britain"
author: "Group 50, Ella Park (s2311400), Saioa Galvin (s2516907), Kieran Marguerie de Rotrou (s2536961), (3,226 words)"
output:
  html_document:
    number_sections: no
  pdf_document:
    number_sections: no
header-includes:
  - \newcommand{\bm}[1]{\boldsymbol{#1}}
  - \newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}
---

```{r setup, include = FALSE}
# Set default code chunk options
knitr::opts_chunk$set(
  echo = TRUE,
  eval = TRUE
)

suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(numDeriv))
suppressPackageStartupMessages(library(mvtnorm))
suppressPackageStartupMessages(library(gridExtra))
suppressPackageStartupMessages(library(latex2exp))
suppressPackageStartupMessages(library(kableExtra))
suppressPackageStartupMessages(library(viridis))
suppressPackageStartupMessages(library(zoo))
theme_set(theme_bw(base_size = 9))

demand <- read.csv("SCS_demand_modelling.csv")
hourly_temp <- read.csv("SCS_hourly_temp.csv")

set.seed(12345L)
```

```{r reading_code, code=readLines("code.R"), eval=TRUE, echo=FALSE}
source("code.R")

```

# Introduction

Accurately forecasting future daily peak electricity demand is crucial for NESO especially during winter when cold weather drives up demand and the risk of shortfalls is highest. To help anticipate extreme scenarios, we focus on modelling the upper tail of the demand distribution to help NESO allocate reserves efficiently.

We use historical daily peak demand data from the `SCS_demand_modelling.csv`, covering winter months from November 1991 to March 2014 across Great Britain. Each row represents a single day, describing electricity demand and capturing key factors that influence this including weather conditions and renewable energy generation.

This report develops and refines a linear regression model to forecast peak demand starting with a baseline model $M_0$ using weather and temporal predictors. Refinements include additional covariates, long-term trends, and improved temperature metrics. To compare and select our best model, we evaluate how our model fits historic data, predictive accuracy and how peak demand varies under different weather conditions. To compare prediction accuracy, we construct a rolling-window cross-validation scheme that computes prediction scores across months and then weekdays versus weekends.

The final model is selected for its predictive performance and interpretability, providing NESO with a reliable forecasting tool. We also highlight limitations and suggest future improvements, such as incorporating recent data and more complex dependencies.

# Data description and exploratory analysis

A thorough understanding of the dataset is essential for effective forecasting. This section  highlights important patterns through visualizations to inform model development and summarizes the pre-analysis steps undertaken.

During preprocessing, we confirmed that there were no missing values in the dataset. Outliers were reviewed for potential input errors, but no evidence warranted their removal, ensuring data quality for model fitting. This was also confirmed when checking the Cook's distance on leverage plots of our final model. The nature of each variable was checked to ensure correct data types (e.g. continuous, categorical) and categorical variables were renamed for easier analysis.

Our analysis begins by analysing how continuous predictors, $wind$, $solar\_S$, $TE$ (temperature) and $year$ relate to the response variable, peak electricity demand ($demand\_gross$).

```{r demand_scatters, echo=FALSE, warning=FALSE}
# Wind vs demand
wind_plot <- ggplot(demand, aes(x=wind, y=demand_gross)) +
  geom_point(alpha=0.5, color=viridis(5)[4]) +
  geom_smooth(method="lm", color="red") +
  labs(title="Peak Demand vs Wind Capacity", x="Wind Capacity Factor", y="Peak Demand (MW)") +
  theme_bw()

# Solar vs demand
solar_plot <- ggplot(demand, aes(x=solar_S, y=demand_gross)) +
  geom_point(alpha=0.5, color=viridis(5)[5]) +
  geom_smooth(method="lm", color="red") +
  labs(title="Peak Demand vs Solar Capacity", x="Solar Capacity Factor", y="Peak Demand (MW)") +
  theme_bw()

# Temp vs demand- shows negative correlation
temp_plot <- ggplot(demand, aes(x=temp, y=demand_gross)) +
   geom_point(alpha=0.5, color=viridis(9)[8]) +
   geom_smooth(method="lm", color="red") +
   labs(title="Peak Demand vs Temperature", x="Temperature (°C)", y="Peak Demand (MW)") +
   theme_bw()

# TE vs demand- shows negative correlation
te_plot <- ggplot(demand, aes(x=TE, y=demand_gross)) +
   geom_point(alpha=0.5, color=viridis(9)[4]) +
   geom_smooth(method="lm", color="red") +
   labs(title="Peak Demand vs TE", x="Temperature (°C)", y="Peak Demand (MW)") +
   theme_bw()

# Start_Year vs demand- shows negative correlation
year_plot <- ggplot(demand, aes(x=start_year, y=demand_gross)) +
   geom_point(alpha=0.5, color=viridis(5)[1]) +
   geom_smooth(method="lm", color="red") +
   labs(title="Peak Demand vs Winter Start Year", x="Year", y="Peak Demand (MW)") +
   theme_bw()
```

```{r demand_scatters_show, warning = FALSE, echo=FALSE, out.width="100%", cache = FALSE, caption = "Plotting peak demand against explanatory variables."}
suppressWarnings(grid.arrange(wind_plot, solar_plot, temp_plot, te_plot, year_plot, ncol = 2))
```

The scatter plot between wind capacity factor and daily peak electricity demand shows a weak relationship, suggesting wind generation has minimal impact on peak demand. This is expected as demand peaks are probably more likely to be influenced by factors like temperature.

The wind variable, though not a strong predictor, was statistically significant in our final model with a regression coefficient of 898.17, even at the 1% significance level. Additionally, we used the $\texttt{step}$ function to perform backward stepwise regression on another of our models. This method iteratively removes variables that do not contribute significantly to the model, based on the Akaike Information Criterion (AIC). Despite its weak relationship, wind remained in the model we conducted this on as its removal did not improve the AIC, indicating it still plays a role in predicting demand.

Most points fall below 0.05, showing little correlation between solar and demand. However, a slight negative correlation appears beyond this threshold, suggesting that higher solar generation reduces peak demand. This is intuitive, as increased solar output reduces electricity needs for lighting supporting the inclusion of solar as an explanatory variable in our model.

Both the scatter plots of temperature and TE against demand show a negative correlation where higher temperatures generally correspond to a lower electricity demand. This relationship is expected, as higher temperatures tend to reduce the need for heating, a significant factor in electricity demand during colder months. Given the clear consistent negative correlation, it was important to include one temperature variable in the model.

Since both temperature and TE are based on similar data, including both could introduce multicollinearity, so we select only one. TE shows slightly less variation in demand compared to temperature as it incorporated data from the previous day, smoothing out the fluctuations. This makes TE a more stable representation of the temperature's influence on demand helping the model to capture long term temperature trends better than temperature alone. This helps to provide the hypothesis that TE is a more valuable measure of temperature than the temp variable. Subsequent analysis will provide further evidence for this.

The plot reveals a correlation, suggesting that demand increases over time. However from this it is evident the relationship is more complex than a simple linear trend, following a cubic pattern. There are a variety of reasons which this occurs such as reflecting long-term trends in energy usage, economic growth, policy changes or technological advancements. The following graphs visualize this hypothesis to assess whether a quadratic or cubic term improves the model's fit compared to the linear one.

```{r exploring_year, echo=FALSE, warning=FALSE}
year_quadratic <- ggplot(demand, aes(x=start_year, y=demand_gross)) +
   geom_point(alpha=0.5, color=viridis(9)[6]) +
   geom_smooth(method="lm", formula = y ~ poly(x, 2), color="red") +  
   labs(title="Peak Demand vs Winter Start Year", x="Winter Start Year", y="Peak Demand (MW)") +
  theme_bw()

year_cubic <- ggplot(demand, aes(x=start_year, y=demand_gross)) +
   geom_point(alpha=0.5, color=viridis(9)[6]) +
   geom_smooth(method="lm", formula = y ~ poly(x, 3), color="red") +  
   labs(title="Peak Demand vs Winter Start Year", x="Winter Start Year", y="Peak Demand (MW)") +
  theme_bw()
```

```{r exploring_year_show, warning = FALSE, echo=FALSE, out.width="100%", cache = FALSE, caption = "Exploring polynomial relationship between peak demand and year"}
grid.arrange(year_quadratic, year_cubic, ncol = 2)
```

Next we investigate the effect of categorical on peak demand by analysing its variation across different days of the week and months using violin plots.

```{r violin, echo=FALSE, warning=FALSE}
# Month vs demand
month_plot <- ggplot(demand, aes(x=factor(monthindex, levels = c(10, 11, 0, 1, 2)), y=demand_gross, fill=factor(monthindex))) +
  geom_violin(alpha=0.5) +  # Create the violin plot
  geom_boxplot(width=0.2, color="black", alpha=0.7) +  # Add a boxplot inside the violin for quartiles
  scale_fill_viridis_d(option="viridis") +    
  scale_x_discrete(labels=c("November", "December", "Janurary", "February", "March")) +
  labs(title="Peak Demand Distribution by Month", x="Month", y="Peak Demand (MW)") +
  theme_bw() +
  guides(fill=FALSE) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Day of week
day_plot <- ggplot(demand, aes(x=factor(wdayindex), y=demand_gross, fill=factor(wdayindex))) +
  geom_violin(alpha=0.5) +  # Create the violin plot
  geom_boxplot(width=0.2, color="black", alpha=0.7) +  # Add a boxplot inside the violin for quartiles
  scale_fill_viridis_d(option="viridis") + 
  scale_x_discrete(labels=c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")) +
  labs(title="Peak Demand by Day of the Week", x="Day of Week", y="Peak Demand (MW)") +
  theme_bw() +
  guides(fill=FALSE) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r violin_show, warning = FALSE, echo=FALSE, out.width="100%", cache = FALSE, caption = "Violin plots to analyse effect of month and day on peak demand."}
grid.arrange(month_plot, day_plot, ncol = 2, nrow = 1)
```

The first plot illustrates the distribution of peak demand across different months. While the peak demand across most months appears relatively consistent, March has a noticeably lower demand. This drop in demand is possibly due to improving weather and decreased heating demand. Given this distinct difference This suggests that the month variable will be an important factor in the model.

The violin plot for day of the week shows that peak electricity demand is generally higher on weekdays than weekends, reflecting increased consumption from offices and factories. 

```{r wday_eda_table, echo=FALSE, warning=FALSE}
weekdays_stats <- data.frame(
  Day = factor(0:6, labels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")),
  Median = tapply(demand$demand_gross, demand$wdayindex, median, na.rm = TRUE),
  Q1 = tapply(demand$demand_gross, demand$wdayindex, function(x) quantile(x, 0.25, na.rm = TRUE)),
  Q3 = tapply(demand$demand_gross, demand$wdayindex, function(x) quantile(x, 0.75, na.rm = TRUE))
)
```

```{r wday_show, warning = FALSE, echo=FALSE, out.width="100%", cache = FALSE, caption = "Median and Quartiles for Each Day of the Week."}
kable(weekdays_stats)
```

While it may seem reasonable to simplify the model by grouping weekends, this overlooks important differences, particularly Friday, which has a median peak nearly 2,000 MW lower than other weekdays. Weekdays overall are fairly consistent, with median peaks around 52,000 MW, compared to about 45,000 MW on weekends. Including day of the week as a factor variable captures these variations without oversimplifying, and later analysis confirms its statistical significance.

The next step is to explore interactions that can help improve the model accuracy. Interactions allow us to capture relationships where the effect of one variable on peak demand depends on the value of another.

```{r interactions, echo=FALSE}
# making daytype and month into factor variables in our dataset
demand$daytype <- factor(
  ifelse(demand$wdayindex %in% 0:4, "Weekday", "Weekend"),
  levels = c("Weekday", "Weekend")
)
demand$month <- factor(demand$monthindex,
  levels = c(10, 11, 0, 1, 2), 
  labels = c("Nov", "Dec", "Jan", "Feb", "Mar")
)
demand$day <- factor(demand$wdayindex,
                     levels = c(0, 1, 2, 3, 4, 5, 6),
                     labels = c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun")
                     )
 solar_wday <- ggplot(demand, aes(x=solar_S, y=demand_gross, color=day)) +
   geom_point(alpha=0.4) +
   geom_smooth(method="lm", se=FALSE) +
   scale_color_viridis_d() +
   labs(title="Interaction Between Solar Generation and Day of the Week", 
        x="Solar Generation", y="Peak Demand (MW)", color="Day of the Week") +
   theme_bw()
 
 solar_month <- ggplot(demand, aes(x=solar_S, y=demand_gross, color=month)) +
   geom_point(alpha=0.4) +
   geom_smooth(method="lm", se=FALSE) +
   scale_color_viridis_d() +
   labs(title="Interaction Between Solar Generation and Month", 
        x="Solar Generation", y="Peak Demand (MW)", color="Month") +
   theme_bw()
 
 TE_month <- ggplot(demand, aes(x=TE, y=demand_gross, color=month)) +
   geom_point(alpha=0.4) +
   geom_smooth(method="lm", se=FALSE) +
   scale_color_viridis_d() +
   labs(title="Interaction Between Temperature and Month", 
        x="Temperature (°C)", y="Peak Demand (MW)", color="Month") +
   theme_bw()
```

```{r interactions_show, warning = FALSE, echo=FALSE, out.width="100%", cache = FALSE, caption = "Analysing interactions between explanatory variables."}
grid.arrange(solar_wday, solar_month, TE_month, ncol = 2, nrow = 2)
```

These plots indicate that interactions should be considered in the model due to varying slopes, and we produced based on this. However, models that excluded interactions ultimately provided better predictive performance. It is difficult to draw conclusions with solar, as there are too few data points above the 0.5 threshold to perform a meaningful analysis visually.

# Model fitting and cross-validation results

## Other Models Considered

To develop a robust forecasting model for daily peak electricity demand, we explored several model formulations and refined them using stepwise selection.

The provided base model, $m0$, accounted for key weather and temporal factors but overlooked categorical nature of both `wdayindex` (day of the week) and `monthindex` (month) which was an inappropriate simplification. We corrected this by modelling them as factors.

Additionally, long-term trends were not captured, despite exploratory analysis suggesting a cubic relationship between demand and year. This insight led to the inclusion of `poly(year, 3)`. This was selected instead over the non-orthogonal polynomial, `year + I(year^2) + I(year^3)` since poly generates orthogonal polynomials ensuring the terms are uncorrelated to reduce multicollinearity.

Based on earlier findings, we also replaced temp with TE as it smoothed out short-term fluctuations and incorporated the lagged effects from previous days' temperatures, providing a more accurate predictor of peak demand.

Our earlier plots also revealed the potential importance of interactions between variables on peak demand. Therefore, it was important our model contained at least two way interactions to accurately capture the relationships and improve predictive power. However, to mitigate the risk of overfitting and reduce the number of unnecessary explanatory variables, we refined the model further by applying backward stepwise regression. This method begins with the full model and iteratively removed the least significant variables. At each step, the model's performance is assessed using the Akaike Information Criterion (AIC), which balances model fit and complexity. The predictor that results in the smallest increase in AIC is removed, and the process continues until no further improvements can be made.

However, our analysis concluded that a simpler model without interactions had better predictive ability than a one without interactions. This led us to the final model This leaves us with the final model:$$
\begin{aligned}
M_1: \text{Peak Demand} &= \beta_0 + \beta_1 \cdot \text{wind} + \beta_2 \cdot \text{solar}_S + \beta_3 \cdot \text{TE} \\
&+ \sum_{i=1}^{6} \beta_{i+3} \cdot \text{day}_i + \sum_{j=1}^{3} \beta_{j+9} \cdot \text{month}_j \\
&+ \beta_{12} \cdot \text{poly(year, 3)}_1 + \beta_{13} \cdot \text{poly(year, 3)}_2 + \beta_{14} \cdot \text{poly(year, 3)}_3 + \varepsilon_i
\end{aligned}
$$ where $\text{demand_gross}_i$ is the peak demand at the time $i$, $\beta_j$ for $j=0,1,\dots,7$ are the regression coefficients and the $\varepsilon\sim\mathcal{N}(0,\sigma^2)$ is the error term at time $i$.

To evaluate the models to consider, the most important metrics to consider are the $R^2$ and Adjusted $R^2$ scores, and Root Mean Squared Error ($RMSE$). These metrics for each model are shown in the following table.

```{r comparing_models, warning = FALSE, echo=FALSE, out.width="50%", cache = FALSE}

# making daytype and month into factor variables in our dataset
demand$daytype <- factor(
  ifelse(demand$wdayindex %in% 0:4, "Weekday", "Weekend"),
  levels = c("Weekday", "Weekend")
)
demand$month <- factor(demand$monthindex,
  levels = c(10, 11, 0, 1, 2), 
  labels = c("Nov", "Dec", "Jan", "Feb", "Mar")
)
demand$day <- factor(demand$wdayindex,
                     levels = c(0, 1, 2, 3, 4, 5, 6),
                     labels = c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun")
                     )

# models we are considering
# final models we are considering
m_0 <- demand_gross ~ wind + solar_S + temp + wdayindex + monthindex

m_0_year <- demand_gross ~ wind + solar_S + temp + day + month + poly(start_year, 3)

m_0_year_te_daytype <- demand_gross ~ wind + solar_S + TE + daytype + month + poly(start_year, 3)

m_prestep <- demand_gross ~ (wind + solar_S + TE + day + month + poly(year, 3))^2

m_backward <- formula(step(lm(m_prestep, demand), direction = "backward", trace = 0))

m_0_year_te <- demand_gross ~ wind + solar_S + TE + day + month + poly(year, 3)

m_aliya <- demand_gross ~ I(wdayindex^2) + monthindex + I(year^3) + 
                  TE + I(DSN^2) + wdayindex + year + DSN + start_year + temp + 
                  TO + wind + TE:year + monthindex:DSN + year:start_year + 
                  DSN:temp + DSN:TO + TE:TO + TE:temp + wdayindex:start_year + 
                  year:DSN + monthindex:year + DSN:start_year + monthindex:start_year + 
                  year:wind


## forward and backward?

# putting all these models into a vector for comparison
formulas <- list(m_0, m_0_year, m_0_year_te_daytype, m_prestep, m_backward, m_0_year_te)
names <- c("m0", "Poly start_year, TE, interactions", "Poly start_year, TE, interactions, poly daytype", "Interactions", "Backward Step on Interactions", "No interactions")

# creating the comparison table
comparison_df <- comparison(formulas, names, demand)
colnames(comparison_df) <- c("Model", "$R^2$", "Adjusted $R^2$", "RMSE")

```

```{r comparison_table, warning = FALSE, echo=FALSE, out.width="100%", cache = FALSE, caption = "Table to compare metrics for perfomance on historical data."}
kable(comparison_df)
```

The final three rows of this table are the most significant, as we are comparing the following models to see the importance of interactions between explanatory variables for our model.

No interactions model:

$$demand\_gross \sim wind + solar\_S + TE + day + month + poly(year, 3) + \varepsilon.$$

All interactions model:

$$demand\_gross \sim (wind + solar\_S + TE + day + month + poly(year, 3))^2 + \varepsilon.$$ Backward step on all interactions model:

$$demand\_gross \sim wind + solar\_S + TE + day + month + poly(year, 3) + \\
wind:month + wind:poly(year, 3) + \\
solar\_S:day + solar\_S:month + \\
TE:month + TE:poly(year, 3) + \\
day:month + day:poly(year, 3) + \\
month:poly(year, 3) + \varepsilon.$$

In this table we are evaluating the results of the model fitting by checking metrics.

The $R^2$ metrics evaluate how well the model fits the response data, written as $R^2 = 1 − \frac{\frac{1}{n} \sum_{i = 1}^n \hat{\varepsilon_i}^2}{\frac{1}{n} \sum_{i = 1}^n (y_i - \bar{y})^2}$. Additionally, we can analyse Adjusted $R^2$, which uses unbiased estimators, because $R^2$ can overestimate how well a model is doing. We have $R_{adj}^2 = 1 − \frac{\frac{1}{n-p} \sum_{i = 1}^n \hat{\varepsilon_i}^2}{\frac{1}{n-1} \sum_{i = 1}^n (y_i - \bar{y})^2}$, where $p$ is the number of model parameters. We want our $R^2$ values to be as close as possible to 1 to indicate a close fit.

The Root Mean Squared Error is defined as $RMSE = \sqrt{\frac{\sum_{i=1}^n(y_i - \hat{y_i})^2}{n - p}}$, where $y_i$ is the actual value for the $i^{th}$ observation, $\hat{y}_i$ is the predicted value for the $i^{th}$ observation, $n$ is the number of observations and $p$ is the number of explanatory variables.

By looking at the table above, we can see that our final model has adjusted $R^2$ near 1, and the interactions cause low $RMSE$. Therefore this supports the evidence that this is a better model than $m0$.

## Testing on Historical Data

As well as metrics, the validity of our model can be shown by testing on historical data.

```{r historical, warning = FALSE, echo=FALSE, out.width="50%", cache = FALSE}

# predicting for just m0 and our best model
m0_prediction <- lm_predicting(m_0, demand)

final_prediction <- lm_predicting(m_0_year_te, demand)

# creating a dataset with predicted and actual data
m0_data <- cbind(m0_prediction, demand)
final_data <- cbind(final_prediction, demand)

# time?
# pick an explanatory variable

m0_historical_daytype <- ggplot(m0_data, aes(x = demand_gross, y = mean_pred, color = daytype)) +
  geom_point(alpha = 0.2) +
  geom_smooth(method = lm, se = TRUE) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +
  labs(
    title = "Modelling m0 For Historical Demand",  # Title
    x = "Actual Demand (Gross)",  # X-axis label
    y = "Predicted Demand (Mean)",  # Y-axis label
    color = "Day type"  # Color legend label
  )

final_historical_daytype <- ggplot(final_data, aes(x = demand_gross, y = mean_pred, color = daytype)) +
  geom_point(alpha = 0.2) +
  geom_smooth(method = lm, se = TRUE) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +
  labs(
    title = "Final Model For Historical Demand",  # Title
    x = "Actual Demand (Gross)",  # X-axis label
    y = "Predicted Demand (Mean)",  # Y-axis label
    color = "Day type"  # Color legend label
  )

m0_historical_month <- ggplot(m0_data, aes(x = demand_gross, y = mean_pred, color = month)) +
  geom_point(alpha = 0.2) +
  geom_smooth(method = lm, se = TRUE) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +
  labs(
    title = "Modelling m0 For Historical Demand",  # Title
    x = "Actual Demand (Gross)",  # X-axis label
    y = "Predicted Demand (Mean)",  # Y-axis label
    color = "Month"  # Color legend label
  )

final_historical_month <- ggplot(final_data, aes(x = demand_gross, y = mean_pred, color = month)) +
  geom_point(alpha = 0.2) +
  geom_smooth(method = lm, se = TRUE) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +
  labs(
    title = "Final Model For Historical Demand",  # Title
    x = "Actual Demand (Gross)",  # X-axis label
    y = "Predicted Demand (Mean)",  # Y-axis label
    color = "Month"  # Color legend label
  )
```

```{r historical_plot, warning = FALSE, echo = FALSE, out.width="100%", cache = FALSE}
grid.arrange(m0_historical_daytype, final_historical_daytype, m0_historical_month, final_historical_month, ncol = 2)
```

These plots show that our final model is a much better fit on historical data than $m0$, as the best-fit lines are all closer to the $y=x$ line.

### Residuals

To check the modelling assumptions of the residuals for our model, we consider the following graphs.

```{r residuals, warning = FALSE, echo=FALSE, out.width="100%", cache = FALSE}
par(mfrow = c(2, 2))
plot(lm(m_0_year_te, data = demand))

```

The Residuals vs Fitted plot shows little change of variance with mean, so the constant variance assumption is upheld. The Q-Q Residuals plot the ordered standardised residuals against quantiles of a standard normal. For smaller sample size, the normality assumption is violated, but we can apply the central limit theorem in this case. Therefore this deviation from the normal line is not very significant. The values on the Scale-Location plot shows random variation around the mean, confirming no violation of the constant variance assumption. Finally, the residuals vs leverage plot gives us Cook’s distance, which measures the change in all model fitted values on omission of the data point in question. Here, it is not problematic, so we can conclude that there are no outliers that need investigating for our model.

## Cross-Validation

In addition to testing on historical data, we test the final model on future data using rolling-window cross-validation.

First, it is important to note that our data now has `daytype` and `month` as factor variables. As we are using time-series data, we perform rolling-window cross-validation training the model on 3 years and testing on the subsequent year, shifting the window forward annually. This compares the basic model, $m_0$ with the final model across winter months.

```{r loocv, warning = FALSE, echo=FALSE, out.width="50%", cache = FALSE}
source("code.R")
m0_scores <- monthly_rolling_model(demand, m_0)
m0_scores$model <- "m0"

prestep_scores <- monthly_rolling_model(demand, m_prestep)
prestep_scores$model <- "Interactions"

backward_scores <- monthly_rolling_model(demand, m_backward)
backward_scores$model <- "Backward"

final_scores <- monthly_rolling_model(demand, m_0_year_te)
final_scores$model <- "No Interactions (final)"

all_scores <- rbind(m0_scores, prestep_scores, backward_scores, final_scores)

score_summary <- all_scores %>%
  group_by(model) %>%
  summarise(mean_se = mean(se),
            mean_ds = mean(ds),
            mean_int = mean(int)) %>%
  ungroup()

colnames(score_summary) <- c("Model", "Mean SE", "Mean DS", "Mean Interval Score")

```

```{r scores_table, warning = FALSE, echo=FALSE, out.width="100%", cache = FALSE, caption="Score comparison of models with respect to month."}
kable(score_summary)
```

This table shows the average Squared Error ($SE$), average Dawid-Sebastiani score ($DS$) and the average Interval score for the basic model and our final model. As these are negatively-oriented scores, smaller values indicate better performance.

Squared Error $S_{SE}(F, y) = (y − \hat{y}_F)^2$ measures the difference from the mean but provides no insight into uncertainty, making differences in SE less important. On the other hand, the Dawid-Sebastiani score $S_{DS}(F, y) = \frac{(y−\mu_F)^2}{\sigma_F^2} + \log(\sigma_F^2)$ accounts for prediction variance. Finally, the Interval score $S_{INT}(F, y) = U_F − L_F + \frac{2}{\alpha}(L_F - y)\mathbb{I}(y < L_F ) + \frac{2}{\alpha}(y − U_F)\mathbb{I}(y > U_F)$ shows us the coverage of our prediction. We want the difference between $U_F$ and $L_F$, the upper and lower bounds for our prediction F, to be as small as possible while still covering the value as much as possible.

These scores justify the simpler model without interactions, as it shows better prediction accuracy despite having lower adjusted $R^2$ compared to more complex models like "Interactions" and "Backward".

We can also examine graphs to see how the score varies by month.

```{r bar_plotting_month, warning = FALSE, echo=FALSE, out.width="50%", cache = FALSE}

monthly_summary <- all_scores %>%
  group_by(model, month) %>%
  summarise(mean_se = mean(se),
            mean_ds = mean(ds),
            mean_int = mean(int)) %>%
  ungroup()

se_month <- ggplot(monthly_summary, aes(x = month, y = mean_se, fill = model)) +
  geom_bar(stat = "identity", position = "dodge") +  # Create a bar chart
  labs(
    title = "Mean SE by Month",
    x = "Month",
    y = "Mean SE"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

ds_month <- ggplot(monthly_summary, aes(x = month, y = mean_ds, fill = model)) +
  geom_bar(stat = "identity", position = "dodge") +  # Create a bar chart
  labs(
    title = "Mean DS by Month",
    x = "Month",
    y = "Mean DS"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

int_month <- ggplot(monthly_summary, aes(x = month, y = mean_int, fill = model)) +
  geom_bar(stat = "identity", position = "dodge") +  # Create a bar chart
  labs(
    title = "Mean Interval Score by Month",
    x = "Month",
    y = "Mean Interval Score"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

```{r bar_plot_month, warning = FALSE, echo=FALSE, cache = FALSE, out.width="100%", caption = "Model scores comparison with respect to month."}
grid.arrange(se_month, ds_month, int_month, ncol = 2)
```

These graphs show that the interaction models are significantly worse for predicting particularly, the model with all interactions is much worse in March. Therefore, we will focus on "m0", and "Final" for subsequent analysis.

```{r reg_plotting_month,  warning = FALSE, echo=FALSE, out.width="50%", cache = FALSE}

ii_scores <- rbind(m0_scores, final_scores)

ii_plot_month <- ggplot(ii_scores, aes(x = actual, y = mean, color = model)) +
  geom_point(alpha = 0.4) +                      # Scatter plot of actual vs predicted
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +  # Line y=x for reference
  facet_wrap(~month) +
  labs(
    title = "Actual vs Predicted Demand for Months",
    x = "Actual Demand",
    y = "Predicted Demand",
    color = "Model"
    )

```

```{r reg_plot_month, warning = FALSE, echo=FALSE, cache = FALSE, out.width="100%", caption = "Model prediction comparison with respect to month."}
ii_plot_month
```

This regression plot compares Actual vs Predicted Demand for our models after rolling-window cross-validation. Points along the black dashed $y=x$ line indicate accurate predictions. The plots reveal that our final model performs far better than $m0$ and better in November and February, with tighter point clusters, while it struggles more in December, January, and March.

We can also assess performance across weekend and weekday.

```{r reg_plotting_daytype,  warning = FALSE, echo=FALSE, out.width="50%", cache = FALSE}

# plotting actual and mean vs daytype for a single model at a time
reg_plot_daytype <- ggplot(ii_scores, aes(x = actual, y = mean, color = model)) +
  geom_point(alpha = 0.4) +                      # Scatter plot of actual vs predicted
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +  # Line y=x for reference
  facet_wrap(~daytype) +
  #ylim(2*10^4, 8*10^4) +
  labs(
    title = "Actual vs Predicted Demand for Day Type",
    x = "Actual Demand",
    y = "Predicted Demand",
    color = "Day type"
    )
```

```{r reg_plot_daytype, warning = FALSE, echo=FALSE, cache = FALSE, out.width="100%", caption = "Model prediction comparison with respect to day type."}
reg_plot_daytype
```

These plots show that our final model outperforms $m0$ for predicting weekdays, though the difference is less pronounced for weekends, possibly due to limited weekend data. To investigate further, we will analyze the scores in more detail.

```{r scores_plotting,  warning = FALSE, echo=FALSE, cache = FALSE}
se_plot <- ggplot(ii_scores, aes(
  x = actual, 
  y = se, 
  color = model
  )) + 
  geom_point() +
  labs(
    title = "Actual vs Squared Error",
    x = "Actual Demand",
    y = "SE",
    color = "Model"
    )

ds_plot <- ggplot(ii_scores, aes(
  x = actual, 
  y = ds, 
  color = model
  )) +
  geom_point() +
  labs(
    title = "Actual vs Dawid-Sebastiani Score",
    x = "Actual Demand",
    y = "DS",
    color = "Model"
    )
```

```{r scores_plot, warning = FALSE, echo = FALSE, cache = FALSE, out.width="80%", caption = "Relationship between scores and demand_gross values."}
se_plot
ds_plot
```

These plots show that the majority of scores for each value of $demand\_gross$  are better using our model than those for $m0$. Additionally, lower scores for higher gross demand indicate that our model is better at predicting extreme values.

## Exchangeability Test

To test if our final model is better at predicting than $m0$, we compare their prediction scores (SE and DS) by randomly swapping them within each cross-validation pair $(S_i^A, S_i^B)$. We use the test statistic $\frac{1}{N} \sum_{i=1}^n(S_i^A - S_i^B)$, and use a Monte Carlo approach to estimate the p-value for a test of exchangeability of predictions, with the alternative hypothesis that our model is superior to $m0$.

```{r exchange, warning = FALSE, echo=FALSE, out.width="50%", cache = FALSE}

# create a dataframe to find the difference between scores for different models

m0_scores$m0_se <- m0_scores$se
m0_scores$m0_ds <- m0_scores$ds

final_scores$inter_se <- final_scores$se
final_scores$inter_ds <- final_scores$ds

scores_diff <- left_join(m0_scores, final_scores, by = c("year", "month", "daytype")) %>%
  summarise(
    se_diff = m0_se - inter_se,
    ds_diff = m0_ds - inter_ds
  )

statistic0 <- scores_diff %>% 
  summarise(se = mean(se_diff), 
            ds = mean(ds_diff))


# number of iterations
J <- 10000

# initialise dataframe
statistic <- data.frame(se = numeric(J),
                        ds = numeric(J))

# loop over randomisation samples
for (loop in seq_len(J)) {
  
  # sample random sign changes
  random_sign <- sample(c(-1, 1), 
                        size = nrow(scores_diff), 
                        replace = TRUE)
  
  statistic[loop, ] <- scores_diff %>% 
    summarise(se = mean(random_sign * se_diff),
              ds = mean(random_sign * ds_diff))
}

# now find the p values
p_values <- statistic %>%
  summarise(se = mean(se > statistic0$se),
            ds = mean(ds > statistic0$ds))

rownames(p_values) <- c("p-values")

```

```{r diff, warning = FALSE, echo = TRUE, cache = FALSE, out.width="50%", caption = "Difference in scores between m0 and our final model."}
kable(head(scores_diff))
```

```{r pvals, warning = FALSE, echo = FALSE, cache = FALSE, out.width="50%", caption = "P-values for hypothesis that the models are exchangeable."}
kable(p_values)
```

We can see that our models are so different that they are not exchangeable. Therefore there is evidence to reject the null hypothesis.

## Maximum Annual Demand Variation

We test how the maximum annual demand for the 2013-2014 winter season changes based on weather conditions from previous winters. Using data from each previous winter season, we simulate the 2013 maximum annual demand for 2013 by predicting values with our model.

```{r Max_Annual_Demand, warning = FALSE, echo = FALSE, out.width = "50%", cache = FALSE}

# First convert character Date to proper date format
demand <- demand %>%
  mutate(
    Date = as.Date(Date),  # Convert character to Date
    day_month = format(Date, "%m-%d"),  # Extract month-day as "MM-DD"
    day_of_month = lubridate::mday(Date) # Eg numbers 1-31
  )

# Prepare the base model
# Using your original model specification
demand_model <- lm(demand_gross ~ wind + solar_S + TE + month + day + poly(year, 3), data = demand)

# Create 2013-14 temporal structure (without weather)
demand_2013_structure <- demand %>%
  filter(start_year == 2013) %>%
  dplyr::select(Date, day_month, day_of_month, day, month, start_year, year)

# Get historical weather data (all years except 2013-14)
historical_weather <- demand %>%
  filter(start_year < 2013) %>%
  dplyr::select(day_month, wind, solar_S, TE)

# Run simulations for all historical years
historical_years <- unique(demand$start_year[demand$start_year < 2013])

# Run simulations (using map_df for efficiency)
simulation_results <- purrr::map_df(historical_years, simulate_max_demand)

# Add actual 2013-14 maximum for comparison
actual_max_2013 <- demand %>%
  filter(start_year == 2013) %>%
  summarise(
    simulated_year = 2013,
    weather_year = 2013,
    max_demand = max(demand_gross, na.rm = TRUE))
simulation_results <- bind_rows(simulation_results, actual_max_2013)

buffer <- 0.01 * actual_max_2013$max_demand  # 10% buffer
y_min <- min(simulation_results$max_demand, actual_max_2013$max_demand) - buffer
y_max <- max(simulation_results$max_demand, actual_max_2013$max_demand) + buffer

# Plot max_demand for each weather year against actual 2013 max_demand
max_demand_plot <- ggplot(simulation_results, aes(x = as.factor(weather_year), y = max_demand)) +
  geom_bar(stat = "identity", fill = "steelblue") +  # Bar plot for simulated demand
  geom_hline(yintercept = actual_max_2013$max_demand, color = "red", linetype = "dashed", size = 1) +  # Red dashed line for actual 2013 max_demand
  labs(
    title = "Maximum Annual Demand under Different Weather Conditions",
    x = "Winter Year",
    y = "Maximum Demand (MW)"
  ) +
  coord_cartesian(ylim = c(y_min, y_max)) +  # Adjust y-axis range
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels
```

```{r plot_max_annual_demand, warning = FALSE, echo=FALSE, out.width="100%", cache = FALSE}
max_demand_plot
```

The plot shows that the maximum demand varies across weather years, ranging from around 51,800 MW (2006) to 54,750 MW (2010). Most years (except 1994, 2006, and 2011) predict higher demand than the actual 2013 value, possibly due to decreasing annual demand in recent years.

One possible explanation is that older weather years reflect colder and harsher winters, which would lead to increased heating demand. In contrast, the 2013/2014 winter may have been milder, potentially due to climate change, resulting in a lower actual peak demand. The overestimation of demand could also reflect shifts towards more energy-conscious behaviour and improved efficiency. Additionally, the decline in electricity demand in later years, driven by technological advancements, may explain why older weather conditions overpredict demand for the 2013/2014 season. Finally particularly high predictions for certain years (such as 2010) likely reflect extreme weather events, highlighting how sensitive peak demand is to severe conditions.

## Alternative temperature variable 

Temperature is a key driver of electricity demand, especially during winter, when colder conditions increase heating usage. NESO currently use TEa rolling average of the previous day's TO (3pm-6pm temperature), to account for temperature in demand forecasting. To assess whether TE is optimal, we first compared it to using TO directly in the model.

Using TO as the temperature variable resulted in a lower adjusted $R^2$ suggesting that daily temperature alone does not fully explain demand variation, whereas TE had an adjusted $R^2$ of 77.24%. This suggests that energy use responds to patterns of cold weather over several days, not just one day's temperature.

We also experimented calculating TO over both a larger and shorter time frame (1pm-6pm and 5pm-6pm) for the TE variable, which resulted in lower $R^2$ values compared to using our current TE variable. Additionally, we explored rolling averages over 2, 3, and 5 days to investigate how incorporating more historical temperature data into the TE variable affects the model. As the window size increased, the adjusted $R^2$ decreased suggesting that adding too much past data dilutes the immediate temperature effects on demand.

```{r temp range, echo=FALSE, warning=FALSE}
# Call the function and get the results for rolling averages over 2, 3, and 5 days
rolling_r2_results <- calculate_rolling_r2(demand, c(2, 3, 5))
colnames(rolling_r2_results) <- c("Time Frame (Days)", "Adjusted $R^2$")
```

```{r temp_comp, warning = FALSE, echo=FALSE, out.width="100%", cache = FALSE}
suppressWarnings(kable(rolling_r2_results))
```
All models incorporating alternative TE variables exhibited lower adjusted $R^2$ than the original that explained 77.24% of the data. This indicates that our current TE calculation provides the most explanatory power, as it better captures the variance in demand relative to the other variations considered.

## Model limitations

Our model relies on a linear regression framework, which carries several assumptions. We have assumed that the response variable, `demand_gross` is normally distributed, and that errors, $\varepsilon_i \sim \mathcal{N}(0, \sigma^2)$, are independent and identically distributed with constant variance (homoscedasticity). However, these assumptions can be problematic in a time-series context, where both the mean and variance of the data may change over time. Our explanatory variables may also not be independent, and by ignoring interactions between them, we could be underfitting our data. Nonetheless, residual plots indicate that the linear model performs reasonably well for the data at hand, with no major violations of assumptions.

Additionally, our model is constrained by the quality of the data itself. As we are only using one dataset, there could be missing explanatory variables that haven't been taken into account. For example, our model aims to predict energy demand across the whole of the UK, but usage will vary from region-to-region, and be especially different in larger cities compared to remote areas. Our model could also consider bottlenecks such as Christmas, may experience unusual spikes in demand that the model does not account for explicitly. With our current dataset, we only consider winter months, which means the model is much less accurate for predicting energy demand year-round.

Environmental and societal changes also present challenges. Variables like $solar_S$ and $temp$ are influenced by ongoing climate change, and shifts in consumer behaviour, energy efficiency, and policy may mean that historical demand data, especially from earlier years, are less representative of current or future patterns. Incorporating more recent data could help address these issues and improve model relevance.

Lastly, while linear regression is computationally efficient and interpretable, it may not scale well to larger or more complex datasets. While it is performing well with prediction here, it could be unsuited to larger datasets as the electricity system evolves. NESO may benefit from exploring more flexible modelling techniques such as random forests or other machine learning techniques, which can better handle non-linear relationships and interactions without overfitting.

# Conclusion

Our analysis finds that a simpler model is preferred for prediction, as including interactions results in over-fitting. This is shown by comparing our final model with both a full interaction model and a backward stepwise regression model. While interaction terms can capture complex relationships between variables, they also substantially increase the number of parameters, making the model more sensitive to noise in the training data. This likely explains why the interaction model, despite fitting the historical data closely, performed worse in external prediction.

In contrast, our model achieves a strong balance between accuracy and interpretability. By incorporating historical demand, temperature (including lagged values via the TE variable), and seasonal effects such as day of week and month, our model outperforms a baseline model. Although our model is less accurate in December and March, which may be due to greater variability and extreme temperatures during these transitional months; December is typically colder, and March marks a shift to warmer weather. Therefore, NESO could benefit from different models for each month, as the difference in scores over the months, as shown in our plots, are significant.

Though the model excludes interaction terms, it still achieves a relatively high adjusted $R^2$ indicating a good fit to historical data. While excluding interactions may underfit subtle relationships, it is computationally less intensive for NESO to run, while maintaining high prediction accuracy. However, the assumption of independence among predictors may limit the model’s scalability as more complex or higher-resolution data become available in future. Exploring selective interactions or nonlinear effects could be a worthwhile following investigation.

Our exploratory data analysis supported the selection of key covariates. Temperature, solar, wind generation, and calendar effects showed strong associations with demand, both intuitively and statistically. For example, weekdays consistently had higher peak demand than weekends, solar generation showed an inverse relationship with demand, and TE emerged as a smoother and more informative temperature measure than daily temperature alone. 

By performing rolling-window cross-validation, we have considered the time-series nature of the data, making sure not to train on future data to test on past data. As our model has the lowest mean DS score of 17.03901, it has high prediction accuracy. This supports NESO's long-term planning goals. By focusing only on winter months, the period of greatest risk for electricity shortfalls, we ensure the model is most effective where it matters most. This will mitigate the possibility that daily peak demand will exceed maximum supply capacity. By analysing our prediction by month, our final model will enable NESO to anticipate shifts in electricity demand, supporting informed decision-making.

# Code Appendix

```{r code=readLines("code.R"), eval=FALSE, echo=TRUE}
# Do not change this code chunk
```
